---
title: "Traffic Data Analysis"
output: html_document
---

```{r}
library(ggplot2)
library(dplyr)
library(lubridate)
```

#Data Reading
```{r}
df <- read.csv(file="../traffic-data.csv", head=TRUE, sep=",")
```

# Data Preparation

Check for duplicate rows 
```{r}
nrow(df) - nrow(unique(df))
```

Remove Columns which has one unique value  
```{r}
df <- Filter(function(x)(length(unique(x))>2), df)
ncol(df)
```

Check mean of NA in each colmun
```{r}
na_means = c()
for (i in 1:ncol(df)) {
     na_means  <- c(na_means,  mean(is.na(df[i])))
}
na_means
```

Last two columns have big NA mean. by looking in the dataset it appeas to be images which can be excluded from the anaylsis. So, its okay to remove these two columns
```{r}
df = subset(df, select = -c(ncol(df), ncol(df)-1) )
ncol(df)
```

Check mean of unique values percentage in each colmun
```{r}
unique_values_percentage = c()
for (i in 1:ncol(df)) {
     val = length(unique(df[i])) / length(df[i])
     unique_values_percentage  <- c(unique_values_percentage,  val)
}
#unique_values_percentage
```
After examining the meaning of data, we can get rid of the duplicate rows(have the same comment id)
```{r}
nrow(df)
df <- subset(df, !duplicated(df[,13]))
nrow(df)
```

Change crawl date column into R-data type date
```{r}
df$crawl_date <- as.POSIXct(df$crawl_date,format = "%a %b  %e %H:%M:%S UTC %Y", tz ="UTC")
```

Normalize the report and road dates
```{r}
df$report_date <-  as.POSIXct(df$crawl_date - (df$rd.rp.hr*60*60) - (df$rd.rp.mn*60))
df$road_date <-  as.POSIXct(df$crawl_date - (df$rd.hr*60*60) - (df$rd.mn*60))
df$rd.rp.hr <-  df$report_date %>% hour() 
df$rd.rp.mn <-  df$report_date %>% minute() 
```

Add columns area, from, to  which repreents the main area of the road and the directions
```{r}
extract_area_and_direction <- function(road_name){
  tokens <- road_name %>% as.character() %>% sapply(function(x){
      a <- x%>% strsplit(split = ';') %>% unlist() %>% trimws()
      b <- a[2] %>% strsplit(split = 'To') %>% unlist() %>% trimws()
      c(a[1], b[1], b[2])
  })
   data.frame(
    rd.area = tokens[1,],
    rd.from = tokens[2,],
    rd.to = tokens[3,]
  )
}

extracted_values <-  extract_area_and_direction(df$rd.nm)
df$rd.area <- extracted_values[,1]
df$rd.from <- extracted_values[,2]
df$rd.to <- extracted_values[,3]
```

The proposed metric to check the congestion of a road is to take the average of the status road(7alawa,laziz, mashy,za7ma,mafesh 2amal) in all the reports concerning that road. But to avoid the outliers to affect the results, the congestion value multipled by the weight of that road.

Here are the first 6 area with highst congestion:

```{r}
crowded_area <- df %>% subset(rd.rp.stid >= 1 & rd.rp.stid <=5 ) %>% group_by(rd.area) %>% summarise(crowded = mean(rd.rp.stid) , count = length(rd.area))
crowded_roads <- df %>% subset(rd.rp.stid >= 1 & rd.rp.stid <=5 ) %>% group_by(rd.nm) %>% summarise(crowded = mean(rd.rp.stid), count = length(rd.nm))

length <- df %>% subset(rd.rp.stid >= 1 & rd.rp.stid <=5 ) %>% nrow()
crowded_area$crowded = crowded_area$crowded * (crowded_area$count/length)
crowded_area <- crowded_area[order(crowded_area$crowded, decreasing = TRUE),]

crowded_roads$crowded = crowded_roads$crowded * (crowded_roads$count/length)
crowded_roads <- crowded_roads[order(crowded_roads$crowded, decreasing = TRUE),]
```

The most crowded areas are:
```{r fig.width=15, fig.height=6, echo=FALSE}
c <- ggplot(head(crowded_area), aes(x = rd.area, y = crowded))
c + geom_bar(stat = "identity", width= 0.5, fill = "red") + xlab("Areas Names") + ylab("Congestion Value")
```

The most croweded roads are:
```{r fig.width=15, fig.height=5, echo=FALSE}
cr <- ggplot(head(crowded_roads), aes(x = rd.nm, y = crowded))
cr + geom_bar(stat = "identity", width= 0.5, fill = "red") + xlab("Roads Names") + ylab("Congestion Value")
```

It is clear that the reports are more distributed in some main areas that should be investigated:
```{r fig.width=15, fig.height=5, echo=FALSE}
reports_denisty <- df %>% group_by(rd.area) %>% summarise(count = length(rd.area))
reports_denisty <- reports_denisty[order(crowded_roads$count, decreasing = TRUE),]
g <- ggplot(head(reports_denisty), aes(x = rd.area, y = count))
g + geom_bar(stat = "identity", width= 0.5, fill = "red") + xlab("Roads Names") + ylab("# of reports")
```


It is resnoable that the rush hour is when the users tend to report more about the roads. So, the proposed metric for the rush hour is to check number of reports each hour.

```{r fig.width=15, fig.height=5, echo=FALSE}
rush_hours <- df %>% group_by(rd.rp.hr) %>% summarise(crowded =  length(rd.rp.hr))
cr <- ggplot(rush_hours, aes(x = rd.rp.hr, y = crowded))
cr + geom_bar(stat = "identity", width= 0.5, fill = "red") + xlab("Hour") + ylab("# of reports")
```
